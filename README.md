# AINews

- Haarnoja T, Moran B, Lever G, et al. **Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning**[J]. arXiv preprint arXiv:2304.13653, 2023. [**[Paper Link]**](https://arxiv.org/abs/2304.13653)

- Remedios L W, Cai L Y, Remedios S W, et al. **Exploring shared memory architectures for end-to-end gigapixel deep learning**[J]. arXiv preprint arXiv:2304.12149, 2023. [**[Paper Link]**](https://arxiv.org/abs/2304.12149v1)

- Hansen-Estruch P, Kostrikov I, Janner M, et al. **IDQL: Implicit Q-Learning as an Actor-Critic Method with Diffusion Policies**[J]. arXiv preprint arXiv:2304.10573, 2023. [**[Paper Link]**](https://arxiv.org/abs/2304.10573)

- Yu C, Zheng X, Zhuo H H, et al. **Reinforcement Learning with Knowledge Representation and Reasoning: A Brief Survey**[J]. arXiv preprint arXiv:2304.12090, 2023. [**[Paper Link]**](https://arxiv.org/abs/2304.12090v1)

- Zhou H, Sui A, Shi L. **Penalty-Based Imitation Learning With Cross Semantics Generation Sensor Fusion for Autonomous Driving**[J]. arXiv preprint arXiv:2303.11888, 2023. [**[Paper Link]**](https://arxiv.org/abs/2303.11888v1)

- Chowdhery A, Narang S, Devlin J, et al. **Palm: Scaling language modeling with pathways**[J]. arXiv preprint arXiv:2204.02311, 2022. [**[Paper Link]**](https://arxiv.org/abs/2204.02311)

- Han Y, Razaviyayn M, Xu R. **Policy Gradient Converges to the Globally Optimal Policy for Nearly Linear-Quadratic Regulators**[J]. arXiv preprint arXiv:2303.08431, 2023. [**[Paper Link]**](https://arxiv.org/abs/2303.08431v1)

- Lu C, Schroecker Y, Gu A, et al. **Structured State Space Models for In-Context Reinforcement Learning**[J]. arXiv preprint arXiv:2303.03982, 2023. [**[Paper Link]**](https://arxiv.org/abs/2303.03982)

- Trockman A, Kolter J Z. **Patches are all you need?**[J]. arXiv preprint arXiv:2201.09792, 2022. [**[Paper Link]**](https://openreview.net/forum?id=rAnB7JSMXL)

- Barber D. **Smoothed Q-learning**[J]. arXiv preprint arXiv:2303.08631, 2023. [**[Paper Link]**](https://arxiv.org/abs/2303.08631v1)

- Abbas Z, Zhao R, Modayil J, et al. **Loss of Plasticity in Continual Deep Reinforcement Learning**[J]. arXiv preprint arXiv:2303.07507, 2023. [**[Paper Link]**](https://arxiv.org/abs/2303.07507)

- Nguyen A, Karampatziakis N, Chen W. Meet in the Middle: **A New Pre-training Paradigm**[J]. arXiv preprint arXiv:2303.07295, 2023. [**[Paper Link]**](https://arxiv.org/abs/2303.07295).

- Wei C, Wang Y C, Wang B, et al. **An Overview on Language Models: Recent Developments and Outlook**[J]. arXiv preprint arXiv:2303.05759, 2023. [**[Paper Link]**](https://arxiv.org/abs/2303.05759).
 
- Introducing a framework to create AI agents that can understand human instructions and perform actions in open-ended settings [[blog link]](https://www.deepmind.com/blog/building-interactive-agents-in-video-game-worlds)

- Jiang M, Rockt√§schel T, Grefenstette E. **General Intelligence Requires Rethinking Exploration**[J]. arXiv preprint arXiv:2211.07819, 2022. [**[Paper Link]**](https://arxiv.org/abs/2211.07819).

- Korbak T, Perez E, Buckley C L. **RL with KL penalties is better viewed as Bayesian inference**[J]. arXiv preprint arXiv:2205.11275, 2022. [**[Paper Link]**](https://arxiv.org/pdf/2205.11275.pdf).

- Gupta A, Pacchiano A, Zhai Y, et al. **Unpacking reward shaping: Understanding the benefits of reward engineering on sample complexity**[J]. Advances in Neural Information Processing Systems, 2022, 35: 15281-15295. [**[Paper Link]**](https://arxiv.org/abs/2210.09579).

- Gehring J, Gopinath D, Won J, et al. **Leveraging Demonstrations with Latent Space Priors**[J]. arXiv preprint arXiv:2210.14685, 2022. [**[Paper Link]**](https://facebookresearch.github.io/latent-space-priors/).

- Gilbert T K, Dean S, Lambert N, et al.**Reward reports for reinforcement learning**[J]. arXiv preprint arXiv:2204.10817, 2022. [**[Paper Link]**](https://arxiv.org/abs/2204.10817).

- Fernandes P, Madaan A, Liu E, et al. **Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation**[J]. arXiv preprint arXiv:2305.00955, 2023. [**[Paper Link]**](https://arxiv.org/abs/2305.00955).

- Yang S, Nachum O, Du Y, et al. **Foundation Models for Decision Making: Problems, Methods, and Opportunities**[J]. arXiv preprint arXiv:2303.04129, 2023. [**[Paper Link]**](https://arxiv.org/pdf/2303.04129.pdf).

- Christiano P F, Leike J, Brown T, et al. **Deep reinforcement learning from human preferences**[J]. Advances in neural information processing systems, 2017, 30. [**[Paper Link]**](https://proceedings.neurips.cc/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf).

- Ziegler D M, Stiennon N, Wu J, et al. **Fine-tuning language models from human preferences**[J]. arXiv preprint arXiv:1909.08593, 2019.[**[Paper Link]**](https://arxiv.org/pdf/1909.08593.pdf)

- Ouyang L, Wu J, Jiang X, et al. **Training language models to follow instructions with human feedback**[J]. Advances in Neural Information Processing Systems, 2022, 35: 27730-27744.[**[Paper Link]**](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf)

- Stiennon N, Ouyang L, Wu J, et al. **Learning to summarize with human feedback**[J]. Advances in Neural Information Processing Systems, 2020, 33: 3008-3021.[**[Paper Link]**](https://arxiv.org/pdf/2009.01325.pdf)

- Bai Y, Jones A, Ndousse K, et al. **Training a helpful and harmless assistant with reinforcement learning from human feedback**[J]. arXiv preprint arXiv:2204.05862, 2022.[**[Paper Link]**](https://arxiv.org/pdf/2204.05862)

- Nakano R, Hilton J, Balaji S, et al. **Webgpt: Browser-assisted question-answering with human feedback**[J]. arXiv preprint arXiv:2112.09332, 2021.[**[Paper Link]**](https://arxiv.org/pdf/2112.09332)

- Uesato J, Kushman N, Kumar R, et al. **Solving math word problems with process-and outcome-based feedback**[J]. arXiv preprint arXiv:2211.14275, 2022.[**[Paper Link]**](https://arxiv.org/pdf/2211.14275)

- Deng M, Wang J, Hsieh C P, et al. **Rlprompt: Optimizing discrete text prompts with reinforcement learning**[J]. arXiv preprint arXiv:2205.12548, 2022.[**[Paper Link]**](https://arxiv.org/pdf/2205.12548)

- Carta T, Romac C, Wolf T, et al. **Grounding large language models in interactive environments with online reinforcement learning**[J]. arXiv preprint arXiv:2302.02662, 2023.[**[Paper Link]**](https://arxiv.org/pdf/2302.02662)

- Mezghani L, Bojanowski P, Alahari K, et al. **Think Before You Act: Unified Policy for Interleaving Language Reasoning with Actions**[J]. arXiv preprint arXiv:2304.11063, 2023.[**[Paper Link]**](https://arxiv.org/pdf/2304.11063)

- Ganguli D, Askell A, Schiefer N, et al. **The capacity for moral self-correction in large language models**[J]. arXiv preprint arXiv:2302.07459, 2023. [**[Paper Link]**](https://arxiv.org/abs/2302.07459)

- Li G, Hammoud H A A K, Itani H, et al. **CAMEL: Communicative Agents for" Mind" Exploration of Large Scale Language Model Society**[J]. arXiv preprint arXiv:2303.17760, 2023. [**[Paper Link]**](https://ghli.org/camel.pdf). [**[Chinese Note Link]**](./docs/Papers/CAMEL.md).

